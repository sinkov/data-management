{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark --packages org.postgresql:postgresql:42.4.0\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import date as dt\n",
    "\n",
    "spark = SparkSession.builder.appName('BA_BOT').getOrCreate()\n",
    "\n",
    "spark_postgre = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"BA_BOT\") \\\n",
    "    .config(\"spark.jars\", \"postgresql-42.4.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "params = spark_postgre.read \\\n",
    "     .format(\"jdbc\") \\\n",
    "     .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "     .option(\"user\", \"postgres\") \\\n",
    "     .option(\"password\", \"SMM695\") \\\n",
    "     .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "\n",
    "\n",
    " df_0 = params \\\n",
    "     .option(\"dbtable\", \"commit_info\") \\\n",
    "     .load()\n",
    "\n",
    " df_1 = params \\\n",
    "     .option(\"dbtable\", \"update_info\") \\\n",
    "     .load()\n",
    "\n",
    " df_2 = params \\\n",
    "     .option(\"dbtable\", \"added_deleted_lines\") \\\n",
    "     .load()\n",
    "\n",
    " df_3 = params \\\n",
    "     .option(\"dbtable\", \"issue\") \\\n",
    "     .load()\n",
    "\n",
    " df_4 = params \\\n",
    "     .option(\"dbtable\", \"comment\") \\\n",
    "     .load()\n",
    "\n",
    " df_0.printSchema() # commit_info TABLE\n",
    " df_1.printSchema() # update_info TABLE\n",
    " df_2.printSchema() # added_deleted_lines TABLE\n",
    " df_3.printSchema() # issue TABLE\n",
    " df_4.printSchema() # comment TABLE\n",
    "\n",
    "df_3 = df_3.withColumn('lifespan_days', F.datediff(df_3.closed_at, df_3.created_at)) # add a new column lifespan: how long the issue existed\n",
    "df_3.show()\n",
    "\n",
    "df_0.describe(['deletions','insertions','lines','files']).show()\n",
    "df_1.describe(['deleted_lines','nloc','complexity','token_count']).show()\n",
    "df_3.describe(['n_comments','project','lifespan_days']).show()\n",
    "\n",
    "\n",
    "#df_0.createOrReplaceTempView('commit_info')\n",
    "#df_1.createOrReplaceTempView('update_info')\n",
    "#df_2.createOrReplaceTempView('added_deleted_lines')\n",
    "\n",
    "# deal with df_3 issue TABLE: perform a logistic regression on project ~ n_comments + age, where age = AGE(closed_at, created_at).int(?)\n",
    "df_3 = df_3.na.drop('any')\n",
    "\n",
    "df_3.describe(['n_comments','project','lifespan_days']).show()\n",
    "\n",
    "df_3 = df_3.withColumn('lifespan_days', F.datediff(df_3.closed_at, df_3.created_at)) # add a new column lifespan: how long the issue existed\n",
    "df_3.show()\n",
    "\n",
    "df_3.groupby('project').count().show() # so pytorch: 120 -> 112, tensorflow: 5358 -> 2005\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "# indexing \"project\"\n",
    "idx_0 = StringIndexer().setInputCol(\"project\").setOutputCol(\"project_idx\")\n",
    "df_3 = idx_0.fit(df_3).transform(df_3)\n",
    "df_3.select(['lifespan_days', 'project', 'project_idx']).show()\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "# one hot encoding for project\n",
    "ohe_0 = OneHotEncoder().setInputCol(\"project_idx\").setOutputCol(\"project_idx_ohe\")\n",
    "ohe_0.fit(df_3).transform(df_3).select(['project', 'project_idx', 'project_idx_ohe']).show()\n",
    "df_3 = ohe_0.fit(df_3).transform(df_3)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Assembling a Vector for Logistic Regression\n",
    "v_0 = VectorAssembler() \\\n",
    "     .setInputCols([\"lifespan_days\", \"project_idx_ohe\"]) \\\n",
    "     .setOutputCol('features_0')\n",
    "df_3 = v_0.transform(df_3)\n",
    "df_3.select(['features_0']).show()\n",
    "\n",
    "# split data\n",
    "train, test = df_3.randomSplit([0.7, 0.3])\n",
    "\n",
    "# take a sample out to make it less imbalanced ---------------------------------------------------\n",
    "df_t = df_3.filter(\"project like '%tensorflow%'\")\n",
    "df_p = df_3.filter(\"project like '%pytorch%'\") \n",
    "big_train_t, small_train_t = df_t.randomSplit([0.95, 0.05]) \n",
    "big_train_p, small_train_p = df_p.randomSplit([0.95, 0.05])\n",
    "# so use small_train_t, big_train_p to train the model and original test to test\n",
    "train_balanced = small_train_t.union(big_train_p)\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "train_balanced.groupBy(train_balanced.project).count().show() #1:1\n",
    "\n",
    "a = lr_0.explainParams().split('\\n')\n",
    "x = 1\n",
    "for i in a:\n",
    "    b = i.split(':',1)\n",
    "    b_0,b_1 = '\\033[1m' + b[0] + '\\033[0m', b[1]\n",
    "    print(\"\"\"{}. {} : {}\n",
    "    \"\"\".format(x, b_0,b_1), flush=True)\n",
    "    x=x+1\n",
    "fitLr_0 = lr_0.fit(train_balanced)\n",
    "fitLr_0.transform(test).select(\"project_idx\", \"prediction\").show()\n",
    "predictions = fitLr_0.transform(test)\n",
    "predictions.select('project_idx', 'rawPrediction', 'probability', 'prediction').toPandas().head(30)\n",
    "s_0 = fitLr_0.summary\n",
    "objectiveHistory = s_0.objectiveHistory\n",
    "print(\"\"\"\n",
    "- Accuracy: {} \n",
    "- Area Under ROC : {}\n",
    "- False Positive Rate by Label: {}\n",
    "- Precision by Label: {}\n",
    "- Tot. Iterations: {}\n",
    "- Objective History: \n",
    "{}\n",
    "\"\"\".format(s_0.accuracy, s_0.areaUnderROC,\n",
    "           s_0.falsePositiveRateByLabel, s_0.precisionByLabel,\n",
    "           s_0.totalIterations, [obj for obj in objectiveHistory]),\n",
    "      flush=True)\n",
    "\n",
    "#from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "#evaluator = BinaryClassificationEvaluator()\n",
    "#print('Test set ROC', evaluator.evaluate(predictions))\n",
    "\n",
    "accuracy = predictions.filter(predictions.project_idx == predictions.prediction).count() / float(predictions.count()) # 1!!!\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "# logistic regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr_0 = LogisticRegression(labelCol=\"project_idx\",featuresCol=\"features_0\")\n",
    "\n",
    "a = lr_0.explainParams().split('\\n')\n",
    "x = 1\n",
    "for i in a:\n",
    "    b = i.split(':',1)\n",
    "    b_0,b_1 = '\\033[1m' + b[0] + '\\033[0m', b[1]\n",
    "    print(\"\"\"{}. {} : {}\n",
    "    \"\"\".format(x, b_0,b_1), flush=True)\n",
    "    x=x+1\n",
    "fitLr_0 = lr_0.fit(train)\n",
    "print(\"\"\" \n",
    "\n",
    "Coefficients:\n",
    "============\n",
    "{}\n",
    "\n",
    "\n",
    "Intercept:\n",
    "=========\n",
    "{}\n",
    "\n",
    "\"\"\".format(fitLr_0.coefficients, fitLr_0.intercept), flush=True)\n",
    "fitLr_0.transform(test).select(\"project_idx\", \"prediction\").show()\n",
    "s_0 = fitLr_0.summary\n",
    "objectiveHistory = s_0.objectiveHistory\n",
    "print(\"\"\"\n",
    "- Accuracy: {} # 1! which is amazing??\n",
    "- Area Under ROC : {}\n",
    "- False Positive Rate by Label: {}\n",
    "- Precision by Label: {}\n",
    "- Tot. Iterations: {}\n",
    "- Objective History: \n",
    "{}\n",
    "\"\"\".format(s_0.accuracy, s_0.areaUnderROC,\n",
    "           s_0.falsePositiveRateByLabel, s_0.precisionByLabel,\n",
    "           s_0.totalIterations, [obj for obj in objectiveHistory]),\n",
    "      flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3e5d06dac1aea09eb84cbae644958a2da07a5326ecbe4c93ba7c637fe317177"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dataManagement')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
